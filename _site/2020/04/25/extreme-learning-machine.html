<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction to extreme learning machine | Miguel Rueda</title>
<meta name="generator" content="Jekyll v4.1.0" />
<meta property="og:title" content="Introduction to extreme learning machine" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An introduction to Extreme learning machine." />
<meta property="og:description" content="An introduction to Extreme learning machine." />
<link rel="canonical" href="http://localhost:4000/2020/04/25/extreme-learning-machine.html" />
<meta property="og:url" content="http://localhost:4000/2020/04/25/extreme-learning-machine.html" />
<meta property="og:site_name" content="Miguel Rueda" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-25T17:47:58-03:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Introduction to extreme learning machine","dateModified":"2020-04-25T17:47:58-03:00","datePublished":"2020-04-25T17:47:58-03:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/04/25/extreme-learning-machine.html"},"url":"http://localhost:4000/2020/04/25/extreme-learning-machine.html","description":"An introduction to Extreme learning machine.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Miguel Rueda" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Miguel Rueda</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to extreme learning machine</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-04-25T17:47:58-03:00" itemprop="datePublished">Apr 25, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!-- https://jekyllrb.com/tutorials/using-jekyll-with-bundler/ -->
<p>Extreme learning machine (ELM) is a supervised learning framework that simplifies the training process of Single Hidden Layer Feedforward Neural Networks (SLFN). This framework was proposed by <a href="https://www.ntu.edu.sg/home/egbhuang/" title="Extreme learning machine">Huang</a> to provide better generalization performance at extremely fast learning speed. It has been exhaustively proved that multilayer perceptron (MLP) networks with only one hidden layer can, sufficiently, approximate any continuous function giving origin to SLFNs, however, this does not guarantee optimal learning time, and generalization capabilities, and ease of implementation. SLFNs are  composed of  three separate layers as shown in figure below.</p>

<figure>
  <img src="http://localhost:4000/assets/img/elm/elm_architecture_git.png" />
</figure>

<p>The input layer is the a set of  m independent and identically distributed (i.i.d) pattern data represented by source nodes. The hidden layer is a set of n nodes where a nonlinear transform is applied to the weighted sum of the input layer data. At the output layer another weighted sum is applied, and usually this weights are updated during training as well.</p>

<p>Compared to SLFN-ELM, traditional supervised learning methods have shown that learning speed of feedforward networks are slower than required. Most of this traditional methods are either gradient based (e.g. backpropagation algortihm) or evolutionary algorithms. The former, could be approached as unrestricted nonlinear optimization problem; where the bottleneck lies on the existence of several local minima, with the underlying assumption of multi-modal nature of the loss function; the latter are rather global optimization problems inspired by biological evolution.</p>

<p>ELMâ€™s learning algorithm randomly chooses and fixes the weights between input and hidden layer, and then analytically determines the weights of the output layer via Least mean squares (LMS) estimators. The fact that the ELM has few parameters to be tuned lead to excel generalization capabilities. The supervised training is occurs as follows:</p>

<p>The SLFN-ELM could be interpreted as linear system</p>
<figure>
  <img src="http://localhost:4000/assets/img/elm/linear_elm.png" />
</figure>

<figure>
  <img src="http://localhost:4000/assets/img/elm/huang_notation.png" />
</figure>

<figure>
  <img src="http://localhost:4000/assets/img/elm/H_matrix_elm.png" />
</figure>

<figure>
  <img src="http://localhost:4000/assets/img/elm/activation_function.png" />
</figure>

<figure>
  <img src="http://localhost:4000/assets/img/elm/beta_solution.png" />
</figure>
<p>Min L2-norm output layer weight:</p>
<figure>
  <img src="http://localhost:4000/assets/img/elm/norm_l2.png" />
</figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">Yd</span><span class="p">,</span> <span class="n">nh</span><span class="p">):</span>
        <span class="s">'''
        X_t: Input pattern
        Y_d: Label
        n_h: Hidden nodes
        '''</span>
        <span class="c1"># # Fixing random state for reproducibility
</span>        <span class="c1"># np.random.seed(7)
</span>        <span class="n">ne</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Xt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Yd</span><span class="p">)</span>    
        <span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">Xt</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># divide by 10 in order to improve convergence
</span>        <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">ne</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nh</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span>
        <span class="n">Hi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Hi</span><span class="p">)</span>        
        <span class="n">Bi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">H</span><span class="p">),</span> <span class="n">Yd</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">Bi</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="s">'''
        ELM test unit for prediction
        X_t: test input pattern
        W_i: Weights vector
        B_i: Bias vector
        '''</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
        <span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">Xt</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">Hi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Hi</span><span class="p">)</span> 
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">Y</span> 
</code></pre></div></div>
<p>So first, in order to check the algorithm that we just have implemented, a simple but useful hard test is the exclusive disjunction function or XOR, which is defined as a not linearly separable function. Consider two patterns:</p>

<figure>
  <img src="http://localhost:4000/assets/img/elm/XOR_eq.png" />
</figure>
<p>Shown in figure bellow is the spatial representation of XOR function for two classes that  cannot be separated in a linear manner.</p>
<figure>
  <img src="http://localhost:4000/assets/img/elm/xor_plot.png" />
</figure>
<p>In order to overcome the limitations of linear separabilityit is possible to map this problem in a SLFN with 4 nodes in the hidden layer.</p>
<figure>
  <img src="http://localhost:4000/assets/img/elm/xor_elm_architecture_git.png" />
</figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#pattern dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]])</span>
<span class="c1">#Fixing random state for reproducibility
</span><span class="n">ELM</span> <span class="o">=</span> <span class="n">ELM</span><span class="p">()</span>
<span class="c1"># training step using 4 nodes in the hidden layer
</span><span class="n">W_i</span><span class="p">,</span> <span class="n">B_i</span> <span class="o">=</span> <span class="n">ELM</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># label prediction using the calculated weigths
</span><span class="n">ELM</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">W_i</span><span class="p">,</span><span class="n">B_i</span><span class="p">)</span>
</code></pre></div></div>

  </div><a class="u-url" href="/2020/04/25/extreme-learning-machine.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Miguel Rueda</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Miguel Rueda</li><li><a class="u-email" href="mailto:makquel@gmail.com">makquel@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/makquel"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">makquel</span></a></li><li><a href="https://www.twitter.com/makquel"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">makquel</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Some computer vision, machine learning and deep learning algorithms  to play around.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
